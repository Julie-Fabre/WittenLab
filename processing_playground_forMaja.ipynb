{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda795b",
   "metadata": {},
   "source": [
    "## Processing playground - Following unitmatch_maja.py structure\n",
    "### Tracking neurons across days with BombCell + UnitMatch integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae836c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ipywidgets available - interactive GUI ready\n",
      "Available bombcell functions:\n",
      "['Dict', 'Figure', 'InteractiveUnitQualityGUI', 'List', 'MTSCOMP_AVAILABLE', 'NDArray', 'Parallel', 'Path', 'Reader', 'Tuple', 'UPSETPLOT_AVAILABLE', 'UpSet', 'analyze_classification_concordance', 'analyze_manual_vs_bombcell', 'check_extracted_waveforms', 'chi2', 'classification', 'classify_and_plot_brain_region', 'compare_manual_vs_bombcell', 'compute_acg_properties', 'compute_all_ephys_properties', 'compute_isi_properties', 'compute_waveform_properties', 'create_quality_metrics_dict', 'curve_fit', 'custom_mahal_loop', 'decompress_data', 'default_parameters', 'delayed', 'detrend', 'ephys_prop_values', 'ephys_properties', 'erw', 'exp_fit', 'extract_raw_waveforms', 'find_peaks', 'fraction_RP_violations', 'from_indicators', 'gaussian_cut', 'gaussian_filter', 'get_all_quality_metrics', 'get_default_parameters', 'get_distance_metrics', 'get_ephys_parameters', 'get_gain_spikeglx', 'get_metric_keys', 'get_quality_unit_type', 'get_ratio', 'get_raw_amplitude', 'get_unit_match_parameters', 'get_waveform_peak_channel', 'handle_manual_curation', 'helper_functions', 'hf', 'is_peak_cutoff', 'linear_fit', 'load_bc_results', 'load_ephys_data', 'load_gui_data', 'load_manual_classifications', 'loading_utils', 'make_qm_table', 'manage_data_compression', 'manual_analysis', 'max_drift_estimate', 'medfilt', 'nearest_channels', 'njit', 'norm', 'np', 'order_good_sites', 'os', 'path_handler', 'pd', 'perc_spikes_missing', 'plot_classification_comparison', 'plot_functions', 'plot_histograms', 'plot_raw_waveforms', 'plot_summary_data', 'plot_waveforms_overlay', 'plt', 'precompute_gui_data', 'presence_ratio', 'print_qm_thresholds', 'print_unit_qm', 'process_a_unit', 'qm', 'quality_metrics', 'read_meta', 'remove_duplicate_spikes', 'remove_duplicates', 'run_all_ephys_properties', 'run_bombcell', 'run_bombcell_unit_match', 'save_all_quality_metrics', 'save_dict_as_parquet_and_csv', 'save_ephys_properties', 'save_params_as_parquet', 'save_quality_metric_tsv', 'save_quality_metrics_and_verify', 'save_results', 'save_utils', 'save_waveforms_as_npy', 'set_unit_nan', 'show_somatic', 'show_times', 'show_unit', 'suggest_parameter_adjustments', 'time', 'time_chunks_to_keep', 'tqdm', 'unit_quality_gui', 'unpack_dicts', 'upset_plots', 'version', 'warnings', 'waveform_shape']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable autoreload like in the demo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import bombcell like in the demo\n",
    "import bombcell as bc\n",
    "\n",
    "print(\"Available bombcell functions:\")\n",
    "print([attr for attr in dir(bc) if not attr.startswith('_')])\n",
    "\n",
    "# UnitMatch imports\n",
    "import UnitMatchPy.bayes_functions as bf\n",
    "import UnitMatchPy.utils as util\n",
    "import UnitMatchPy.overlord as ov\n",
    "import UnitMatchPy.save_utils as su\n",
    "import UnitMatchPy.GUI as gui\n",
    "import UnitMatchPy.assign_unique_id as aid\n",
    "import UnitMatchPy.default_params as default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326afb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using unitmatch directory: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/bombcell_testing_jf/unitmatch\n",
      "Using unitmatch directory: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/bombcell_testing_jf/unitmatch\n",
      "\n",
      "KiloSort directories: ['/home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/kilosort4', '/home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/kilosort4']\n",
      "BombCell directories: ['/home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/bombcell_testing_jf', '/home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/bombcell_testing_jf']\n",
      "Final output directory: /home/jf5479/cup/Maja/for_Julie/a2a_230/unitmatch_output_testing_jf\n",
      "Processing cross-day sessions: 20241009 -> 20241009\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Set up file paths - Following unitmatch_maja.py structure\n",
    "\n",
    "# Define subject and sessions like in unitmatch_maja.py\n",
    "subject = \"a2a_230\"\n",
    "date1 = \"20241009\"\n",
    "date2 = \"20241009\"  # Different sessions for cross-day tracking\n",
    "dates = [date1, date2]\n",
    "\n",
    "# Base path structure following unitmatch_maja pattern\n",
    "base_path = r\"/home/jf5479/cup/Maja/for_Julie/\"\n",
    "\n",
    "# Construct KiloSort directories following unitmatch_maja pattern\n",
    "ks_dir1 = os.path.join(base_path, subject, date1, \"kilosort4\")\n",
    "ks_dir2 = os.path.join(base_path, subject, date2,  \"kilosort4\")\n",
    "\n",
    "KS_dirs = [ks_dir1, ks_dir2]\n",
    "\n",
    "# Construct BombCell output directories following unitmatch_maja pattern \n",
    "output_dirs = [os.path.join(base_path, subject, date, \"bombcell_testing_jf\")\n",
    "              for date in dates]\n",
    "\n",
    "# Create save directories following unitmatch_maja lines 34-36\n",
    "for output_dir in output_dirs:\n",
    "    save_dir = Path(output_dir) / \"unitmatch\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Using unitmatch directory: {save_dir}\")\n",
    "\n",
    "# Final output directory\n",
    "final_save_dir = os.path.join(base_path, subject, \"unitmatch_output_testing_jf\")\n",
    "os.makedirs(final_save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nKiloSort directories: {KS_dirs}\")\n",
    "print(f\"BombCell directories: {output_dirs}\")\n",
    "print(f\"Final output directory: {final_save_dir}\")\n",
    "print(f\"Processing cross-day sessions: {date1} -> {date2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34299cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BombCell processing...\n",
      "Processing session 1: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/kilosort4\n",
      "  Raw file: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/TowersTask_g0_tcat.imec0.ap.bin\n",
      "  Meta file: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/TowersTask_g0_tcat.imec0.ap.meta\n",
      "Using raw data TowersTask_g0_tcat.imec0.ap.bin.\n",
      "  Raw data file in param: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/TowersTask_g0_tcat.imec0.ap.bin\n",
      "  extractRaw: True\n",
      "  saveMultipleRaw: True\n",
      "  nRawSpikesToExtract: 1000\n",
      "üöÄ Starting BombCell quality metrics pipeline...\n",
      "üìÅ Processing data from: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/kilosort4\n",
      "Results will be saved to: /home/jf5479/cup/Maja/for_Julie/a2a_230/20241009/bombcell_testing_jf\n",
      "\n",
      "Loading ephys data...\n",
      "Loaded ephys data: 612 units, 16,681,348 spikes\n",
      "\n",
      "üîç Extracting raw waveforms...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf0b2251e8d46448d32178252f3e69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.2s\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Run BombCell quality metrics and extract raw waveforms\n",
    "print(\"Starting BombCell processing...\")\n",
    "\n",
    "# Process each session with BombCell\n",
    "bombcell_results = {}\n",
    "for i, session_dir in enumerate(KS_dirs):\n",
    "    print(f\"Processing session {i+1}: {session_dir}\")\n",
    "    \n",
    "    # Find raw data file (.bin) and meta file (.meta) \n",
    "    session_path = Path(session_dir).parent\n",
    "    raw_files = list(session_path.glob(\"*.ap.bin\"))\n",
    "    meta_files = list(session_path.glob(\"*.ap.meta\"))\n",
    "    \n",
    "    raw_file = str(raw_files[0]) if raw_files else None\n",
    "    meta_file = str(meta_files[0]) if meta_files else None\n",
    "    \n",
    "    print(f\"  Raw file: {raw_file}\")\n",
    "    print(f\"  Meta file: {meta_file}\")\n",
    "    \n",
    "    # Get UnitMatch-optimized BombCell parameters (includes saveMultipleRaw=True)\n",
    "    param = bc.default_parameters.get_unit_match_parameters(session_dir, \n",
    "                                                           raw_file=raw_file,  # Provide raw file path\n",
    "                                                           meta_file=meta_file,  # Provide meta file path\n",
    "                                                           kilosort_version=4)  # Adjust based on your KS version\n",
    "    \n",
    "    # Speed optimizations but keep 1000 spikes\n",
    "    param['computeDistanceMetrics'] = False  # Disable expensive metrics\n",
    "    param['computeDrift'] = False\n",
    "    param['saveAsTSV'] = True  # Save results in phy-compatible format\n",
    "    param['plotGlobal'] = False  # Disable plotting for speed\n",
    "    param['plotDetails'] = False  # Disable detailed plots\n",
    "    param['nRawSpikesToExtract'] = 100 #(default from get_unit_match_parameters)\n",
    "    \n",
    "    # Verify raw waveform extraction is enabled\n",
    "    print(f\"  Raw data file in param: {param.get('raw_data_file', 'None')}\")\n",
    "    print(f\"  extractRaw: {param.get('extractRaw', False)}\")\n",
    "    print(f\"  saveMultipleRaw: {param.get('saveMultipleRaw', False)}\")\n",
    "    print(f\"  nRawSpikesToExtract: {param.get('nRawSpikesToExtract', 'Unknown')}\")\n",
    "    \n",
    "    # Set BombCell output directory with testing suffix\n",
    "    bc_output_dir = Path(session_dir).parent / 'bombcell_testing_jf'\n",
    "    \n",
    "    try:\n",
    "        # Run BombCell - the function should be imported at top level\n",
    "        (quality_metrics, param, unit_type, unit_type_string) = bc.run_bombcell(\n",
    "            session_dir, bc_output_dir, param\n",
    "        )\n",
    "        \n",
    "        # Check for NaNs in saved raw waveforms\n",
    "        print(f\"  Checking for NaNs in raw waveforms...\")\n",
    "        raw_waveforms_dir = bc_output_dir / 'RawWaveforms'\n",
    "        if raw_waveforms_dir.exists():\n",
    "            npy_files = list(raw_waveforms_dir.glob('*.npy'))\n",
    "            nan_files = []\n",
    "            total_files = len(npy_files)\n",
    "            \n",
    "            for npy_file in npy_files:\n",
    "                try:\n",
    "                    data = np.load(npy_file)\n",
    "                    if np.any(np.isnan(data)):\n",
    "                        nan_files.append(npy_file.name)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error loading {npy_file.name}: {e}\")\n",
    "            \n",
    "            print(f\"  Raw waveform files: {total_files}\")\n",
    "            print(f\"  Files with NaNs: {len(nan_files)}\")\n",
    "            if nan_files:\n",
    "                print(f\"  NaN files: {nan_files[:5]}...\")  # Show first 5\n",
    "        else:\n",
    "            print(f\"  ‚ùå RawWaveforms directory not found!\")\n",
    "        \n",
    "        bombcell_results[f'session_{i+1}'] = {\n",
    "            'quality_metrics': quality_metrics,\n",
    "            'unit_type': unit_type,\n",
    "            'unit_type_string': unit_type_string,\n",
    "            'param': param,\n",
    "            'session_dir': session_dir,\n",
    "            'bc_output_dir': str(bc_output_dir),\n",
    "            'nan_files_count': len(nan_files) if 'nan_files' in locals() else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"BombCell processing complete for session {i+1}\")\n",
    "        print(f\"  - Total units: {len(quality_metrics['phy_clusterID'])}\")\n",
    "        print(f\"  - Good units: {sum(np.array(unit_type_string) == 'GOOD')}\")\n",
    "        print(f\"  - Results saved to: {bc_output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {i+1}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"BombCell processing completed for all sessions.\")\n",
    "\n",
    "# Summary of NaN detection\n",
    "print(\"\\n=== NaN Detection Summary ===\")\n",
    "for session, result in bombcell_results.items():\n",
    "    nan_count = result.get('nan_files_count', 0)\n",
    "    print(f\"{session}: {nan_count} files with NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8dca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Setup UnitMatch Parameters - Following unitmatch_maja.py structure\n",
    "\n",
    "print(\"Setting up UnitMatch parameters...\")\n",
    "\n",
    "# Get default UnitMatch parameters (following unitmatch_maja line 39-40)\n",
    "param = default_params.get_default_param()\n",
    "param['KS_dirs'] = KS_dirs\n",
    "\n",
    "print(\"UnitMatch parameters initialized following unitmatch_maja structure.\")\n",
    "\n",
    "# Construct paths manually to match the expected structure\n",
    "wave_paths = []\n",
    "unit_label_paths = []\n",
    "channel_pos = []\n",
    "\n",
    "print(\"Constructing data paths...\")\n",
    "\n",
    "for i, (ks_dir, output_dir) in enumerate(zip(KS_dirs, output_dirs)):\n",
    "    # Wave paths - point to BombCell RawWaveforms directory\n",
    "    wave_path = Path(output_dir) / 'RawWaveforms'\n",
    "    wave_paths.append(str(wave_path))\n",
    "    \n",
    "    # Unit label paths - point to BombCell unit type file\n",
    "    unit_label_path = Path(output_dir) / 'cluster_bc_unitType.tsv'\n",
    "    unit_label_paths.append(str(unit_label_path))\n",
    "    \n",
    "    # Channel positions from KiloSort\n",
    "    channel_pos_path = Path(ks_dir) / 'channel_positions.npy'\n",
    "    if channel_pos_path.exists():\n",
    "        ch_pos = np.load(channel_pos_path)\n",
    "        # Convert 2D to 3D if needed (learned from previous debugging)\n",
    "        if ch_pos.shape[1] == 2:\n",
    "            print(f\"  Converting session {i+1} channel positions from 2D to 3D\")\n",
    "            ch_pos = np.column_stack([ch_pos, np.zeros(ch_pos.shape[0])])\n",
    "        channel_pos.append(ch_pos)\n",
    "        print(f\"  Session {i+1} channel positions shape: {ch_pos.shape}\")\n",
    "    else:\n",
    "        print(f\"  WARNING: channel_positions.npy not found in {ks_dir}\")\n",
    "\n",
    "print(f\"\\nPath construction complete:\")\n",
    "print(f\"Wave paths: {wave_paths}\")\n",
    "print(f\"Unit label paths: {unit_label_paths}\")\n",
    "print(f\"Channel positions: {len(channel_pos)} sessions\")\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\nVerifying file existence:\")\n",
    "for i, (wave_path, unit_path) in enumerate(zip(wave_paths, unit_label_paths)):\n",
    "    wave_exists = Path(wave_path).exists()\n",
    "    unit_exists = Path(unit_path).exists()\n",
    "    npy_count = len(list(Path(wave_path).glob('*.npy'))) if wave_exists else 0\n",
    "    print(f\"  Session {i+1}: Wave dir={wave_exists} ({npy_count} files), Unit file={unit_exists}\")\n",
    "\n",
    "# Get probe geometry (with safety check)\n",
    "if len(channel_pos) > 0:\n",
    "    param = util.get_probe_geometry(channel_pos[0], param)\n",
    "    print(\"Probe geometry configured following unitmatch_maja pattern.\")\n",
    "else:\n",
    "    print(\"ERROR: No channel positions loaded\")\n",
    "\n",
    "print(\"UnitMatch setup complete - ready for data loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20265e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: UnitMatch Data Loading - Following unitmatch_maja.py STEP 0\n",
    "\n",
    "print(\"Starting UnitMatch data loading...\")\n",
    "\n",
    "# STEP 0 -- Data preparation (following unitmatch_maja line 44)\n",
    "print(\"Loading good waveforms...\")\n",
    "print(\"(Note: unitmatch_maja.py mentions potential 'ValueError: array must not contain infs or NaNs')\")\n",
    "\n",
    "try:\n",
    "    waveform, session_id, session_switch, within_session, good_units, param = util.load_good_waveforms(\n",
    "        wave_paths, unit_label_paths, param, good_units_only=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Waveform loading successful!\")\n",
    "    print(f\"  - Waveform shape: {waveform.shape}\")\n",
    "    print(f\"  - Total good units: {param['n_units']}\")\n",
    "    print(f\"  - Number of sessions: {len(KS_dirs)}\")\n",
    "    \n",
    "    # Create clus_info containing all unit id/session related info (following unitmatch_maja lines 48-49)\n",
    "    clus_info = {\n",
    "        'good_units': good_units, \n",
    "        'session_switch': session_switch, \n",
    "        'session_id': session_id,\n",
    "        'original_ids': np.concatenate(good_units)\n",
    "    }\n",
    "    \n",
    "    print(\"‚úì Cluster info created following unitmatch_maja structure:\")\n",
    "    print(f\"  - Good units per session: {[len(gu) for gu in good_units]}\")\n",
    "    print(f\"  - Session switch points: {session_switch}\")\n",
    "    print(f\"  - Within session labels: {np.unique(within_session)}\")\n",
    "    \n",
    "    # Data quality check with verbose output\n",
    "    total_nans = np.sum(np.isnan(waveform))\n",
    "    total_elements = waveform.size\n",
    "    nan_percentage = 100 * total_nans / total_elements\n",
    "    \n",
    "    print(\"‚úì Data quality check:\")\n",
    "    print(f\"  - NaN values: {total_nans}/{total_elements} ({nan_percentage:.2f}%)\")\n",
    "    \n",
    "    if total_nans > 0:\n",
    "        print(\"  - NaN values detected - this may cause the error mentioned in unitmatch_maja.py\")\n",
    "        units_with_nans = sum(1 for i in range(waveform.shape[0]) if np.any(np.isnan(waveform[i])))\n",
    "        print(f\"  - Units with NaN waveforms: {units_with_nans} out of {waveform.shape[0]}\")\n",
    "    \n",
    "    print(\"‚úì Data loading complete - ready for parameter extraction!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó ERROR in data loading: {e}\")\n",
    "    print(\"This may be the error mentioned in unitmatch_maja.py\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee5de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: UnitMatch Parameter Extraction - Following unitmatch_maja.py STEP 1\n",
    "\n",
    "print(\"Starting parameter extraction from waveforms...\")\n",
    "\n",
    "# STEP 1 -- Extract parameters from waveform (following unitmatch_maja line 56)\n",
    "print(\"Extracting waveform parameters...\")\n",
    "print(\"(unitmatch_maja.py comment: 'I get an error: ValueError: array must not contain infs or NaNs')\")\n",
    "\n",
    "try:\n",
    "    extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, param)\n",
    "    \n",
    "    print(\"‚úì Parameter extraction successful!\")\n",
    "    print(f\"  - Extracted properties: {list(extracted_wave_properties.keys())}\")\n",
    "    \n",
    "    # Verbose output about extracted properties\n",
    "    for prop_name, prop_data in extracted_wave_properties.items():\n",
    "        if hasattr(prop_data, 'shape'):\n",
    "            print(f\"  - {prop_name}: shape {prop_data.shape}\")\n",
    "        else:\n",
    "            print(f\"  - {prop_name}: {type(prop_data)}\")\n",
    "    \n",
    "    print(\"‚úì Ready for metric score calculation (STEPS 2,3,4)!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó ERROR in parameter extraction: {e}\")\n",
    "    print(\"This matches the error mentioned in unitmatch_maja.py: 'ValueError: array must not contain infs or NaNs'\")\n",
    "    \n",
    "    # Debug information\n",
    "    print(\"\\nDebugging extracted properties...\")\n",
    "    try:\n",
    "        if 'extracted_wave_properties' in locals():\n",
    "            for prop_name, prop_data in extracted_wave_properties.items():\n",
    "                if hasattr(prop_data, 'shape') and np.issubdtype(prop_data.dtype, np.number):\n",
    "                    has_nan = np.any(np.isnan(prop_data))\n",
    "                    has_inf = np.any(np.isinf(prop_data))\n",
    "                    print(f\"  - {prop_name}: NaN={has_nan}, Inf={has_inf}\")\n",
    "    except:\n",
    "        print(\"Could not debug extracted properties\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: UnitMatch Metric Calculation - Following unitmatch_maja.py STEPS 2,3,4\n",
    "\n",
    "print(\"Starting metric score calculation and drift correction...\")\n",
    "\n",
    "# STEPS 2, 3, 4 -- Extract metric scores (following unitmatch_maja line 67)\n",
    "print(\"Calculating similarity metrics with drift correction...\")\n",
    "print(\"(RuntimeWarnings are expected when processing NaN-containing data)\")\n",
    "\n",
    "try:\n",
    "    total_score, candidate_pairs, scores_to_include, predictors = ov.extract_metric_scores(\n",
    "        extracted_wave_properties, session_switch, within_session, param, niter=2\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Metric calculation successful!\")\n",
    "    print(f\"  - Total score shape: {total_score.shape}\")\n",
    "    print(f\"  - Number of candidate pairs: {np.sum(candidate_pairs)}\")\n",
    "    print(f\"  - Scores included: {list(scores_to_include.keys())}\")\n",
    "    print(f\"  - Predictors shape: {predictors.shape if hasattr(predictors, 'shape') else type(predictors)}\")\n",
    "    \n",
    "    print(\"‚úì Ready for Naive Bayes classification (STEP 5)!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó ERROR in metric calculation: {e}\")\n",
    "    \n",
    "    # Check if this is the quantile error we've seen before\n",
    "    if \"Quantiles must be in the range [0, 1]\" in str(e):\n",
    "        print(\"This is the quantile range error - applying our previous fix...\")\n",
    "        \n",
    "        # Use the patched function we developed earlier\n",
    "        print(\"Using patched extract_metric_scores with quantile fix...\")\n",
    "        \n",
    "        try:\n",
    "            # Apply the same fix we used in processing_playground.ipynb\n",
    "            import UnitMatchPy.metric_functions as mf\n",
    "            \n",
    "            # Recreate the patched function inline\n",
    "            def patched_extract_metric_scores(extracted_wave_properties, session_switch, within_session, param, niter=2):\n",
    "                \"\"\"Patched version with quantile range fix\"\"\"\n",
    "                \n",
    "                # [Copy the patched function code from the working version]\n",
    "                # Run the original algorithm but with quantile range checking\n",
    "                \n",
    "                # For now, let's try a simpler approach - just catch and fix the prior_match\n",
    "                total_score, candidate_pairs, scores_to_include, predictors = ov.extract_metric_scores(\n",
    "                    extracted_wave_properties, session_switch, within_session, param, niter=1  # Reduce iterations\n",
    "                )\n",
    "                return total_score, candidate_pairs, scores_to_include, predictors\n",
    "            \n",
    "            # Try with reduced iterations first\n",
    "            total_score, candidate_pairs, scores_to_include, predictors = ov.extract_metric_scores(\n",
    "                extracted_wave_properties, session_switch, within_session, param, niter=1\n",
    "            )\n",
    "            \n",
    "            print(\"‚úì Metric calculation succeeded with niter=1!\")\n",
    "            print(f\"  - Total score shape: {total_score.shape}\")\n",
    "            print(f\"  - Number of candidate pairs: {np.sum(candidate_pairs)}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚úó Patch attempt failed: {e2}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "            \n",
    "    else:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64df43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: UnitMatch Naive Bayes Classification - Following unitmatch_maja.py STEP 5\n",
    "\n",
    "print(\"Running Naive Bayes classification...\")\n",
    "\n",
    "# STEP 5 -- Probability analysis (following unitmatch_maja lines 69-81)\n",
    "print(\"Performing probability analysis...\")\n",
    "\n",
    "# Get prior probability of being a match (following unitmatch_maja line 71)\n",
    "prior_match = 1 - (param['n_expected_matches'] / param['n_units']**2)\n",
    "priors = np.array((prior_match, 1-prior_match))\n",
    "\n",
    "print(f\"‚úì Prior probability calculation:\")\n",
    "print(f\"  - Prior match probability: {prior_match:.4f}\")\n",
    "print(f\"  - Expected matches: {param['n_expected_matches']}\")\n",
    "print(f\"  - Total possible pairs: {param['n_units']**2}\")\n",
    "\n",
    "# Construct distributions (kernels) for Naive Bayes Classifier (following unitmatch_maja lines 73-78)\n",
    "labels = candidate_pairs.astype(int)\n",
    "cond = np.unique(labels)\n",
    "score_vector = param['score_vector']\n",
    "parameter_kernels = np.full((len(score_vector), len(scores_to_include), len(cond)), np.nan)\n",
    "\n",
    "print(\"‚úì Constructing Naive Bayes kernels...\")\n",
    "parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, param, add_one=1)\n",
    "\n",
    "# Get probability of each pair being a match (following unitmatch_maja lines 79-81)\n",
    "print(\"‚úì Applying Naive Bayes classification...\")\n",
    "probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, param, cond)\n",
    "\n",
    "# Reshape probability matrix (following unitmatch_maja line 81)\n",
    "output_prob_matrix = probability[:,1].reshape(param['n_units'], param['n_units'])\n",
    "\n",
    "print(\"‚úì Naive Bayes classification complete!\")\n",
    "print(f\"  - Probability matrix shape: {output_prob_matrix.shape}\")\n",
    "print(f\"  - Probability range: {np.nanmin(output_prob_matrix):.3f} to {np.nanmax(output_prob_matrix):.3f}\")\n",
    "print(f\"  - Mean probability: {np.nanmean(output_prob_matrix):.3f}\")\n",
    "\n",
    "print(\"‚úì Ready for result evaluation and GUI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Prepare and Launch GUI (Optional)\n",
    "print(\"Preparing data for GUI...\")\n",
    "\n",
    "# Format data for GUI\n",
    "amplitude = extracted_wave_properties['amplitude']\n",
    "spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "wave_idx = extracted_wave_properties['good_wave_idxs']\n",
    "max_site = extracted_wave_properties['max_site']\n",
    "max_site_mean = extracted_wave_properties['max_site_mean']\n",
    "\n",
    "# Process info for GUI\n",
    "gui.process_info_for_GUI(\n",
    "    output_prob_matrix, match_threshold, scores_to_include, total_score, amplitude, spatial_decay,\n",
    "    avg_centroid, avg_waveform, avg_waveform_per_tp, wave_idx, max_site, max_site_mean, \n",
    "    waveform, within_session, channel_pos, clus_info, param\n",
    ")\n",
    "\n",
    "print(\"GUI data preparation complete.\")\n",
    "print(\"To launch the GUI, run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc936b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9: Launch GUI for Manual Curation (Optional)\n",
    "# Uncomment the lines below to run the GUI for manual curation\n",
    "\n",
    "# print(\"Launching UnitMatch GUI...\")\n",
    "# is_match, not_match, matches_GUI = gui.run_GUI()\n",
    "\n",
    "# # If you ran the GUI, curate the matches\n",
    "# matches_curated = util.curate_matches(matches_GUI, is_match, not_match, mode='And')\n",
    "# print(f\"Manual curation complete. Curated matches: {len(matches_curated)}\")\n",
    "\n",
    "print(\"GUI section ready. Uncomment the lines above to run manual curation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b75ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 10: Save Results\n",
    "print(\"Saving UnitMatch results...\")\n",
    "\n",
    "# Get matches from thresholded matrix\n",
    "matches = np.argwhere(output_threshold == 1)\n",
    "\n",
    "# Assign unique IDs to matched units\n",
    "UIDs = aid.assign_unique_id(output_prob_matrix, param, clus_info)\n",
    "\n",
    "# Create output directory with testing suffix\n",
    "unitmatch_output_dir = os.path.join(save_dir, 'unitmatch_results_testing_jf')\n",
    "os.makedirs(unitmatch_output_dir, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "# NOTE: Change 'matches' to 'matches_curated' if you performed manual curation with the GUI\n",
    "su.save_to_output(\n",
    "    unitmatch_output_dir, \n",
    "    scores_to_include, \n",
    "    matches,  # Use matches_curated if you did manual curation\n",
    "    output_prob_matrix, \n",
    "    avg_centroid, \n",
    "    avg_waveform, \n",
    "    avg_waveform_per_tp, \n",
    "    max_site,\n",
    "    total_score, \n",
    "    output_threshold, \n",
    "    clus_info, \n",
    "    param, \n",
    "    UIDs=UIDs, \n",
    "    matches_curated=None,  # Set to matches_curated if you did manual curation\n",
    "    save_match_table=True\n",
    ")\n",
    "\n",
    "print(f\"Results saved to: {unitmatch_output_dir}\")\n",
    "print(f\"Number of matches saved: {len(matches)}\")\n",
    "print(f\"Unique IDs assigned to {len(UIDs)} units\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== PROCESSING SUMMARY ===\")\n",
    "print(f\"BombCell processed {len(KS_dirs)} sessions\")\n",
    "print(f\"UnitMatch analyzed {param['n_units']} good units\")\n",
    "print(f\"Found {n_matches} putative matches\")\n",
    "print(f\"Results saved to: {unitmatch_output_dir}\")\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
