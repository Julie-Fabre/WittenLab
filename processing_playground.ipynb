{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda795b",
   "metadata": {},
   "source": [
    "## Processing playground \n",
    "### tracking neurons across days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae836c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Available bombcell functions:\n",
      "['Dict', 'Figure', 'InteractiveUnitQualityGUI', 'List', 'MTSCOMP_AVAILABLE', 'NDArray', 'Parallel', 'Path', 'Reader', 'Tuple', 'UPSETPLOT_AVAILABLE', 'UpSet', 'analyze_classification_concordance', 'analyze_manual_vs_bombcell', 'check_extracted_waveforms', 'chi2', 'classification', 'classify_and_plot_brain_region', 'compare_manual_vs_bombcell', 'compute_acg_properties', 'compute_all_ephys_properties', 'compute_isi_properties', 'compute_waveform_properties', 'create_quality_metrics_dict', 'curve_fit', 'custom_mahal_loop', 'decompress_data', 'default_parameters', 'delayed', 'detrend', 'ephys_prop_values', 'ephys_properties', 'erw', 'exp_fit', 'extract_raw_waveforms', 'find_peaks', 'fraction_RP_violations', 'from_indicators', 'gaussian_cut', 'gaussian_filter', 'get_all_quality_metrics', 'get_default_parameters', 'get_distance_metrics', 'get_ephys_parameters', 'get_gain_spikeglx', 'get_metric_keys', 'get_quality_unit_type', 'get_ratio', 'get_raw_amplitude', 'get_unit_match_parameters', 'get_waveform_peak_channel', 'handle_manual_curation', 'helper_functions', 'hf', 'is_peak_cutoff', 'linear_fit', 'load_bc_results', 'load_ephys_data', 'load_gui_data', 'load_manual_classifications', 'loading_utils', 'make_qm_table', 'manage_data_compression', 'manual_analysis', 'max_drift_estimate', 'medfilt', 'nearest_channels', 'njit', 'norm', 'np', 'order_good_sites', 'os', 'path_handler', 'pd', 'perc_spikes_missing', 'plot_classification_comparison', 'plot_functions', 'plot_histograms', 'plot_raw_waveforms', 'plot_summary_data', 'plot_waveforms_overlay', 'plt', 'precompute_gui_data', 'presence_ratio', 'print_qm_thresholds', 'print_unit_qm', 'process_a_unit', 'qm', 'quality_metrics', 'read_meta', 'remove_duplicate_spikes', 'remove_duplicates', 'run_all_ephys_properties', 'run_bombcell', 'run_bombcell_unit_match', 'save_all_quality_metrics', 'save_dict_as_parquet_and_csv', 'save_ephys_properties', 'save_params_as_parquet', 'save_quality_metric_tsv', 'save_quality_metrics_and_verify', 'save_results', 'save_utils', 'save_waveforms_as_npy', 'set_unit_nan', 'show_somatic', 'show_times', 'show_unit', 'suggest_parameter_adjustments', 'time', 'time_chunks_to_keep', 'tqdm', 'unit_quality_gui', 'unpack_dicts', 'upset_plots', 'version', 'warnings', 'waveform_shape']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Enable autoreload like in the demo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import bombcell like in the demo\n",
    "import bombcell as bc\n",
    "\n",
    "print(\"Available bombcell functions:\")\n",
    "print([attr for attr in dir(bc) if not attr.startswith('_')])\n",
    "\n",
    "# UnitMatch imports\n",
    "import UnitMatchPy.bayes_functions as bf\n",
    "import UnitMatchPy.utils as util\n",
    "import UnitMatchPy.overlord as ov\n",
    "import UnitMatchPy.save_utils as su\n",
    "import UnitMatchPy.GUI as gui\n",
    "import UnitMatchPy.assign_unique_id as aid\n",
    "import UnitMatchPy.default_params as default_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326afb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KiloSort directories: ['/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4', '/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4']\n",
      "BombCell directories: ['/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf', '/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf']\n",
      "Output directory: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/unitmatch_output_testing_jf\n",
      "\n",
      "NOTE: Using the SAME session twice to test for merges/splits within the session\n",
      "This is correct for validating UnitMatch merge detection capabilities\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Set up file paths\n",
    "# KiloSort directories - USE SAME SESSION TWICE for merge/split testing\n",
    "# loop for animals in cta_backwards directory starting with calca_2 ot calca_3. use two last day folders inside. \n",
    "KS_dirs = [r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4',\n",
    "           r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4']  # Same session repeated\n",
    "\n",
    "# BombCell output directories - USE SAME SESSION TWICE  \n",
    "custom_bombcell_paths = [r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf',\n",
    "                         r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf']  # Same session repeated\n",
    "\n",
    "# Output directory for saving results - add testing suffix\n",
    "save_dir = r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/unitmatch_output_testing_jf'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(f\"KiloSort directories: {KS_dirs}\")\n",
    "print(f\"BombCell directories: {custom_bombcell_paths}\")\n",
    "print(f\"Output directory: {save_dir}\")\n",
    "print(\"\")\n",
    "print(\"NOTE: Using the SAME session twice to test for merges/splits within the session\")\n",
    "print(\"This is correct for validating UnitMatch merge detection capabilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34299cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BombCell processing...\n",
      "Processing session 1: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4\n",
      "  Raw file: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_tcat.imec0.ap.bin\n",
      "  Meta file: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_tcat.imec0.ap.meta\n",
      "Using raw data cz_npxl_g0_t0.imec0.ap.bin.\n",
      "  Raw data file in param: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/cz_npxl_g0_t0.imec0.ap.bin\n",
      "  extractRaw: True\n",
      "  saveMultipleRaw: True\n",
      "  nRawSpikesToExtract: 100\n",
      "ðŸš€ Starting BombCell quality metrics pipeline...\n",
      "ðŸ“ Processing data from: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4\n",
      "Results will be saved to: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf\n",
      "\n",
      "Loading ephys data...\n",
      "Loaded ephys data: 657 units, 12,878,168 spikes\n",
      "\n",
      "ðŸ” Extracting raw waveforms...\n",
      "Loading file /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/templates._bc_rawWaveforms.npy... Done!\n",
      "No splits/merges detected\n",
      "\n",
      "âš™ï¸ Computing quality metrics for 657 units...\n",
      "   (Progress bar will appear below)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c41f826f5240d7a7f5d8d155b63362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing bombcell quality metrics:   0%|          | 0/657 units"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     41\u001b[39m bc_output_dir = Path(session_dir).parent / \u001b[33m'\u001b[39m\u001b[33mbombcell_testing_jf\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# Run BombCell - the function should be imported at top level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     (quality_metrics, param, unit_type, unit_type_string) = \u001b[43mbc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_bombcell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbc_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Check for NaNs in saved raw waveforms\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Checking for NaNs in raw waveforms...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Python/bombcell/py_bombcell/bombcell/helper_functions.py:1163\u001b[39m, in \u001b[36mrun_bombcell\u001b[39m\u001b[34m(ks_dir, save_path, param)\u001b[39m\n\u001b[32m   1160\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâš™ï¸ Computing quality metrics for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_units\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m units...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   (Progress bar will appear below)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m quality_metrics, times = \u001b[43mget_all_quality_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[43munique_templates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspike_times_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspike_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemplate_amplitudes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpc_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpc_features_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquality_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_waveforms_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchannel_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemplate_waveforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m param.get(\u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ·ï¸ Classifying units (good/MUA/noise/non-soma)...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Python/bombcell/py_bombcell/bombcell/helper_functions.py:831\u001b[39m, in \u001b[36mget_all_quality_metrics\u001b[39m\u001b[34m(unique_templates, spike_times_seconds, spike_clusters, template_amplitudes, time_chunks, pc_features, pc_features_idx, quality_metrics, raw_waveforms_full, channel_positions, template_waveforms, param, save_path, gui_data)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;66;03m# get time chunks to keep\u001b[39;00m\n\u001b[32m    823\u001b[39m time_tmp = time.time()\n\u001b[32m    824\u001b[39m (\n\u001b[32m    825\u001b[39m     these_spike_times,\n\u001b[32m    826\u001b[39m     these_amplitudes,\n\u001b[32m    827\u001b[39m     these_spike_clusters,\n\u001b[32m    828\u001b[39m     quality_metrics[\u001b[33m\"\u001b[39m\u001b[33museTheseTimesStart\u001b[39m\u001b[33m\"\u001b[39m][unit_idx],\n\u001b[32m    829\u001b[39m     quality_metrics[\u001b[33m\"\u001b[39m\u001b[33museTheseTimesStop\u001b[39m\u001b[33m\"\u001b[39m][unit_idx],\n\u001b[32m    830\u001b[39m     quality_metrics[\u001b[33m\"\u001b[39m\u001b[33mRPV_window_index\u001b[39m\u001b[33m\"\u001b[39m][unit_idx],\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m ) = \u001b[43mqm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtime_chunks_to_keep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpercent_missing_gaussian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfraction_RPVs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthese_spike_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthese_amplitudes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspike_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspike_times_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m runtimes_chunks_to_keep[unit_idx] = time.time() - time_tmp\n\u001b[32m    843\u001b[39m use_these_times = np.array(\n\u001b[32m    844\u001b[39m     (\n\u001b[32m    845\u001b[39m         quality_metrics[\u001b[33m\"\u001b[39m\u001b[33museTheseTimesStart\u001b[39m\u001b[33m\"\u001b[39m][unit_idx],\n\u001b[32m    846\u001b[39m         quality_metrics[\u001b[33m\"\u001b[39m\u001b[33museTheseTimesStop\u001b[39m\u001b[33m\"\u001b[39m][unit_idx],\n\u001b[32m    847\u001b[39m     )\n\u001b[32m    848\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/Python/bombcell/py_bombcell/bombcell/quality_metrics.py:933\u001b[39m, in \u001b[36mtime_chunks_to_keep\u001b[39m\u001b[34m(percent_missing_gaussian, fraction_RPVs, time_chunks, these_spike_times, these_amplitudes, spike_clusters, spike_times_seconds, param)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# select which ones to keep\u001b[39;00m\n\u001b[32m    927\u001b[39m these_amplitudes = these_amplitudes[\n\u001b[32m    928\u001b[39m     np.logical_and(\n\u001b[32m    929\u001b[39m         these_spike_times >= use_these_times[\u001b[32m0\u001b[39m],\n\u001b[32m    930\u001b[39m         these_spike_times <= use_these_times[-\u001b[32m1\u001b[39m],\n\u001b[32m    931\u001b[39m     )\n\u001b[32m    932\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m these_spike_clusters = spike_clusters.copy().astype(np.int32)\n\u001b[32m    934\u001b[39m these_spike_clusters[\n\u001b[32m    935\u001b[39m     np.logical_or(\n\u001b[32m    936\u001b[39m         spike_times_seconds < use_these_times[\u001b[32m0\u001b[39m], \n\u001b[32m    937\u001b[39m         spike_times_seconds > use_these_times[-\u001b[32m1\u001b[39m],\n\u001b[32m    938\u001b[39m     )\n\u001b[32m    939\u001b[39m ] = -\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# set to -1 for bad times\u001b[39;00m\n\u001b[32m    941\u001b[39m these_spike_times = these_spike_times[\n\u001b[32m    942\u001b[39m     np.logical_and(\n\u001b[32m    943\u001b[39m         these_spike_times >= use_these_times[\u001b[32m0\u001b[39m],\n\u001b[32m    944\u001b[39m         these_spike_times <= use_these_times[-\u001b[32m1\u001b[39m],\n\u001b[32m    945\u001b[39m     )\n\u001b[32m    946\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## Step 2: Run BombCell quality metrics and extract raw waveforms\n",
    "print(\"Starting BombCell processing...\")\n",
    "\n",
    "# Process each session with BombCell\n",
    "bombcell_results = {}\n",
    "for i, session_dir in enumerate(KS_dirs):\n",
    "    print(f\"Processing session {i+1}: {session_dir}\")\n",
    "    \n",
    "    # Find raw data file (.bin) and meta file (.meta) \n",
    "    session_path = Path(session_dir).parent\n",
    "    raw_files = list(session_path.glob(\"*.ap.bin\"))\n",
    "    meta_files = list(session_path.glob(\"*.ap.meta\"))\n",
    "    \n",
    "    raw_file = str(raw_files[0]) if raw_files else None\n",
    "    meta_file = str(meta_files[0]) if meta_files else None\n",
    "    \n",
    "    print(f\"  Raw file: {raw_file}\")\n",
    "    print(f\"  Meta file: {meta_file}\")\n",
    "    \n",
    "    # Get UnitMatch-optimized BombCell parameters (includes saveMultipleRaw=True)\n",
    "    param = bc.default_parameters.get_unit_match_parameters(session_dir, \n",
    "                                                           raw_file=raw_file,  # Provide raw file path\n",
    "                                                           meta_file=meta_file,  # Provide meta file path\n",
    "                                                           kilosort_version=4)  # Adjust based on your KS version\n",
    "    \n",
    "    # Speed optimizations but keep 1000 spikes\n",
    "    param['computeDistanceMetrics'] = False  # Disable expensive metrics\n",
    "    param['computeDrift'] = False\n",
    "    param['saveAsTSV'] = True  # Save results in phy-compatible format\n",
    "    param['plotGlobal'] = False  # Disable plotting for speed\n",
    "    param['plotDetails'] = False  # Disable detailed plots\n",
    "    param['nRawSpikesToExtract'] = 100 #(default from get_unit_match_parameters)\n",
    "    \n",
    "    # Verify raw waveform extraction is enabled\n",
    "    print(f\"  Raw data file in param: {param.get('raw_data_file', 'None')}\")\n",
    "    print(f\"  extractRaw: {param.get('extractRaw', False)}\")\n",
    "    print(f\"  saveMultipleRaw: {param.get('saveMultipleRaw', False)}\")\n",
    "    print(f\"  nRawSpikesToExtract: {param.get('nRawSpikesToExtract', 'Unknown')}\")\n",
    "    \n",
    "    # Set BombCell output directory with testing suffix\n",
    "    bc_output_dir = Path(session_dir).parent / 'bombcell_testing_jf'\n",
    "    \n",
    "    try:\n",
    "        # Run BombCell - the function should be imported at top level\n",
    "        (quality_metrics, param, unit_type, unit_type_string) = bc.run_bombcell(\n",
    "            session_dir, bc_output_dir, param\n",
    "        )\n",
    "        \n",
    "        # Check for NaNs in saved raw waveforms\n",
    "        print(f\"  Checking for NaNs in raw waveforms...\")\n",
    "        raw_waveforms_dir = bc_output_dir / 'RawWaveforms'\n",
    "        if raw_waveforms_dir.exists():\n",
    "            npy_files = list(raw_waveforms_dir.glob('*.npy'))\n",
    "            nan_files = []\n",
    "            total_files = len(npy_files)\n",
    "            \n",
    "            for npy_file in npy_files:\n",
    "                try:\n",
    "                    data = np.load(npy_file)\n",
    "                    if np.any(np.isnan(data)):\n",
    "                        nan_files.append(npy_file.name)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error loading {npy_file.name}: {e}\")\n",
    "            \n",
    "            print(f\"  Raw waveform files: {total_files}\")\n",
    "            print(f\"  Files with NaNs: {len(nan_files)}\")\n",
    "            if nan_files:\n",
    "                print(f\"  NaN files: {nan_files[:5]}...\")  # Show first 5\n",
    "        else:\n",
    "            print(f\"  âŒ RawWaveforms directory not found!\")\n",
    "        \n",
    "        bombcell_results[f'session_{i+1}'] = {\n",
    "            'quality_metrics': quality_metrics,\n",
    "            'unit_type': unit_type,\n",
    "            'unit_type_string': unit_type_string,\n",
    "            'param': param,\n",
    "            'session_dir': session_dir,\n",
    "            'bc_output_dir': str(bc_output_dir),\n",
    "            'nan_files_count': len(nan_files) if 'nan_files' in locals() else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"BombCell processing complete for session {i+1}\")\n",
    "        print(f\"  - Total units: {len(quality_metrics['phy_clusterID'])}\")\n",
    "        print(f\"  - Good units: {sum(np.array(unit_type_string) == 'GOOD')}\")\n",
    "        print(f\"  - Results saved to: {bc_output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {i+1}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"BombCell processing completed for all sessions.\")\n",
    "\n",
    "# Summary of NaN detection\n",
    "print(\"\\n=== NaN Detection Summary ===\")\n",
    "for session, result in bombcell_results.items():\n",
    "    nan_count = result.get('nan_files_count', 0)\n",
    "    print(f\"{session}: {nan_count} files with NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8dca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for UnitMatch...\n",
      "Using predefined BombCell paths...\n",
      "  Session 1: 657 .npy files found in /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/RawWaveforms\n",
      "  Session 2: 657 .npy files found in /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/RawWaveforms\n",
      "BombCell output directories: ['/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf', '/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf']\n",
      "Raw waveform paths: ['/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/RawWaveforms', '/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/RawWaveforms']\n",
      "Unit label paths: ['/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/cluster_bc_unitType.tsv', '/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/cluster_bc_unitType.tsv']\n",
      "Channel positions loaded: 2 sessions\n",
      "  Session 1: Unit label file found: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/cluster_bc_unitType.tsv\n",
      "  Session 2: Unit label file found: /home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf/cluster_bc_unitType.tsv\n",
      "Data preparation for UnitMatch complete.\n"
     ]
    }
   ],
   "source": [
    "## Step 3: Prepare data for UnitMatch\n",
    "print(\"Preparing data for UnitMatch...\")\n",
    "\n",
    "# Since we know the raw waveforms are saved, let's directly use the paths\n",
    "print(\"Using predefined BombCell paths...\")\n",
    "bc_output_dirs = custom_bombcell_paths\n",
    "\n",
    "# Check if directories exist and have RawWaveforms\n",
    "for i, bc_dir in enumerate(bc_output_dirs):\n",
    "    raw_waveforms_dir = Path(bc_dir) / 'RawWaveforms'\n",
    "    if raw_waveforms_dir.exists():\n",
    "        npy_files = list(raw_waveforms_dir.glob('*.npy'))\n",
    "        print(f\"  Session {i+1}: {len(npy_files)} .npy files found in {raw_waveforms_dir}\")\n",
    "    else:\n",
    "        print(f\"  Session {i+1}: RawWaveforms directory not found at {raw_waveforms_dir}\")\n",
    "\n",
    "# Get default UnitMatch parameters\n",
    "param = default_params.get_default_param()\n",
    "\n",
    "# Set up paths for UnitMatch - using the directories defined above\n",
    "param['KS_dirs'] = KS_dirs\n",
    "\n",
    "print(f\"BombCell output directories: {bc_output_dirs}\")\n",
    "\n",
    "# Manually construct the correct paths since util.paths_from_KS isn't working properly\n",
    "wave_paths = []\n",
    "unit_label_paths = []\n",
    "channel_pos = []\n",
    "\n",
    "for i, (ks_dir, bc_dir) in enumerate(zip(KS_dirs, bc_output_dirs)):\n",
    "    # Raw waveforms path\n",
    "    wave_path = Path(bc_dir) / 'RawWaveforms'\n",
    "    wave_paths.append(str(wave_path))\n",
    "    \n",
    "    # Unit label path - point to the specific TSV file\n",
    "    unit_label_path = Path(bc_dir) / 'cluster_bc_unitType.tsv'\n",
    "    unit_label_paths.append(str(unit_label_path))\n",
    "    \n",
    "    # Channel positions from KiloSort\n",
    "    channel_pos_path = Path(ks_dir) / 'channel_positions.npy'\n",
    "    if channel_pos_path.exists():\n",
    "        channel_pos.append(np.load(channel_pos_path))\n",
    "    else:\n",
    "        print(f\"WARNING: channel_positions.npy not found in {ks_dir}\")\n",
    "\n",
    "print(f\"Raw waveform paths: {wave_paths}\")\n",
    "print(f\"Unit label paths: {unit_label_paths}\")\n",
    "print(f\"Channel positions loaded: {len(channel_pos)} sessions\")\n",
    "\n",
    "# Verify the unit label files exist\n",
    "for i, unit_label_path in enumerate(unit_label_paths):\n",
    "    if Path(unit_label_path).exists():\n",
    "        print(f\"  Session {i+1}: Unit label file found: {unit_label_path}\")\n",
    "    else:\n",
    "        print(f\"  Session {i+1}: Unit label file NOT found: {unit_label_path}\")\n",
    "\n",
    "# Get probe geometry\n",
    "if len(channel_pos) > 0:\n",
    "    param = util.get_probe_geometry(channel_pos[0], param)\n",
    "    print(\"Data preparation for UnitMatch complete.\")\n",
    "else:\n",
    "    print(\"ERROR: No channel positions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20265e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting UnitMatch processing...\n",
      "Loading good waveforms...\n",
      "Converting session 1 channel positions from 2D to 3D\n",
      "Session 1 channel positions shape: (384, 3)\n",
      "Converting session 2 channel positions from 2D to 3D\n",
      "Session 2 channel positions shape: (384, 3)\n",
      "Total number of good units: 292\n",
      "Number of sessions: 2\n",
      "Waveform data quality check: 0/13679616 NaN values (0.00%)\n",
      "Extracting waveform parameters...\n",
      "(RuntimeWarnings about NaN slices are expected and handled by UnitMatch)\n",
      "Parameter extraction complete.\n"
     ]
    }
   ],
   "source": [
    "## Step 4: Run UnitMatch - Data Loading and Parameter Extraction\n",
    "\n",
    "# Check if previous step completed successfully\n",
    "if 'wave_paths' not in locals():\n",
    "    print(\"ERROR: wave_paths not defined - UnitMatch preparation failed\")\n",
    "    print(\"Please run the previous cell (Step 3) successfully first\")\n",
    "    print(\"Current local variables:\", [var for var in locals().keys() if not var.startswith('_')])\n",
    "else:\n",
    "    print(\"Starting UnitMatch processing...\")\n",
    "\n",
    "    # STEP 0 -- Data preparation\n",
    "    print(\"Loading good waveforms...\")\n",
    "    waveform, session_id, session_switch, within_session, good_units, param = util.load_good_waveforms(\n",
    "        wave_paths, unit_label_paths, param, good_units_only=True\n",
    "    ) \n",
    "\n",
    "    # Fix channel positions - convert 2D to 3D if needed\n",
    "    for i, ch_pos in enumerate(channel_pos):\n",
    "        if ch_pos.shape[1] == 2:\n",
    "            # Add z=0 dimension for 2D Neuropixels probes\n",
    "            print(f\"Converting session {i+1} channel positions from 2D to 3D\")\n",
    "            channel_pos[i] = np.column_stack([ch_pos, np.zeros(ch_pos.shape[0])])\n",
    "        print(f\"Session {i+1} channel positions shape: {channel_pos[i].shape}\")\n",
    "\n",
    "    # You may need to set peak location if it's not automatically detected correctly\n",
    "    # param['peak_loc'] = # set as a value if the peak location is NOT ~ half the spike width\n",
    "\n",
    "    # Create clus_info containing all unit id/session related info\n",
    "    clus_info = {\n",
    "        'good_units': good_units, \n",
    "        'session_switch': session_switch, \n",
    "        'session_id': session_id, \n",
    "        'original_ids': np.concatenate(good_units)\n",
    "    }\n",
    "\n",
    "    print(f\"Total number of good units: {param['n_units']}\")\n",
    "    print(f\"Number of sessions: {len(KS_dirs)}\")\n",
    "    \n",
    "    # Check for NaNs in waveform data before processing\n",
    "    total_nans = np.sum(np.isnan(waveform))\n",
    "    total_elements = waveform.size\n",
    "    print(f\"Waveform data quality check: {total_nans}/{total_elements} NaN values ({100*total_nans/total_elements:.2f}%)\")\n",
    "    \n",
    "    if total_nans > 0:\n",
    "        print(\"WARNING: Found NaN values in waveform data - this may cause metric calculation warnings\")\n",
    "        # Find which units have NaNs\n",
    "        units_with_nans = []\n",
    "        for i in range(waveform.shape[0]):\n",
    "            if np.any(np.isnan(waveform[i])):\n",
    "                units_with_nans.append(i)\n",
    "        print(f\"Units with NaN waveforms: {len(units_with_nans)} out of {waveform.shape[0]}\")\n",
    "\n",
    "    # STEP 1 -- Extract parameters from waveforms\n",
    "    print(\"Extracting waveform parameters...\")\n",
    "    print(\"(RuntimeWarnings about NaN slices are expected and handled by UnitMatch)\")\n",
    "    extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, param)\n",
    "    print(\"Parameter extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ee5de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity metrics and applying drift correction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:326: RuntimeWarning: divide by zero encountered in divide\n",
      "  ang = np.abs( x1[dim_id1,:,:,:,:] - x2[dim_id1,:,:,:,:]) / np.abs(x1[dim_id2,:,:,:,:] - x2[dim_id2,:,:,:,:])\n",
      "/home/jf5479/anaconda3/envs/ephys/lib/python3.11/site-packages/numpy/lib/_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:147: RuntimeWarning: All-NaN slice encountered\n",
      "  new_vals = np.nanmin(tmpdat, axis =1, keepdims=True) + np.nanmax(tmpdat, axis = 1, keepdims=True) - tmpdat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done drift correction per shank for session pair 1 and 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:211: RuntimeWarning: All-NaN slice encountered\n",
      "  centroid_dist = np.nanmin( euclid_dist[:,new_peak_loc - waveidx ==0,:,:].squeeze(), axis =1 ).squeeze()\n",
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:220: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  centroid_var = np.nanmin( np.nanvar(euclid_dist, axis = 1, ddof = 1 ).squeeze(), axis =1 ).squeeze()\n",
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:220: RuntimeWarning: All-NaN slice encountered\n",
      "  centroid_var = np.nanmin( np.nanvar(euclid_dist, axis = 1, ddof = 1 ).squeeze(), axis =1 ).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug patched function:\n",
      "include_these_pairs length: 2968\n",
      "n_expected_matches: 3595\n",
      "prior_match calculation: 1 - (3595 / 2968) = -0.21125336927223715\n",
      "prior_match value: -0.21125336927223715\n",
      "prior_match type: <class 'numpy.float64'>\n",
      "Is prior_match finite: True\n",
      "total_score shape: (292, 292)\n",
      "include_these_pairs_idx sum: 2968\n",
      "total_score[include_these_pairs_idx.astype(bool)] shape: (2968,)\n",
      "total_score min: 0.1306966085751271\n",
      "total_score max: 1.0\n",
      "Quantile calculation failed: Quantiles must be in the range [0, 1]\n",
      "prior_match is outside [0,1] range!\n",
      "prior_match is negative - using 0.0 instead\n",
      "Quantile calculation with corrected prior_match succeeded: 0.1306966085751271\n",
      "SUCCESS: Number of candidate pairs: 84688\n",
      "Scores included: ['amp_score', 'spatial_decay_score', 'centroid_overlord_score', 'centroid_dist', 'waveform_score', 'trajectory_score']\n",
      "Metric calculation and drift correction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:281: RuntimeWarning: Mean of empty slice\n",
      "  centroid_dist_recentered = np.nanmin( np.nanmean(euclid_dist_2, axis =1), axis =1)\n",
      "/home/jf5479/Dropbox/Python/UnitMatch/UnitMatchPy/UnitMatchPy/metric_functions.py:281: RuntimeWarning: All-NaN slice encountered\n",
      "  centroid_dist_recentered = np.nanmin( np.nanmean(euclid_dist_2, axis =1), axis =1)\n",
      "/tmp/ipykernel_19902/2977382829.py:40: RuntimeWarning: All-NaN slice encountered\n",
      "  euclid_dist = np.nanmin(euclid_dist[:,param['peak_loc'] - param['waveidx'] == 0, :,:].squeeze(), axis=1)\n"
     ]
    }
   ],
   "source": [
    "## Step 5: UnitMatch - Metric Calculation and Drift Correction\n",
    "print(\"Calculating similarity metrics and applying drift correction...\")\n",
    "\n",
    "# Temporarily patch the UnitMatch overlord function to debug the quantile issue\n",
    "import UnitMatchPy.overlord as ov_original\n",
    "import UnitMatchPy.metric_functions as mf\n",
    "\n",
    "def patched_extract_metric_scores(extracted_wave_properties, session_switch, within_session, param, niter=2):\n",
    "    \"\"\"Patched version with debugging for quantile issue\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Unpack needed arrays from the ExtractedWaveProperties dictionary\n",
    "    amplitude = extracted_wave_properties['amplitude']\n",
    "    spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "    spatial_decay_fit = extracted_wave_properties['spatial_decay_fit']\n",
    "    avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "    avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "    avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "\n",
    "    # These scores are NOT affected by the drift correction\n",
    "    amp_score = mf.get_simple_metric(amplitude)\n",
    "    spatial_decay_score = mf.get_simple_metric(spatial_decay)\n",
    "    spatial_decay_fit_score = mf.get_simple_metric(spatial_decay_fit, outlier=True)\n",
    "    wave_corr_score = mf.get_wave_corr(avg_waveform, param)\n",
    "    wave_mse_score = mf.get_waveforms_mse(avg_waveform, param)\n",
    "\n",
    "    # Affected by drift\n",
    "    for i in range(niter):\n",
    "        avg_waveform_per_tp_flip = mf.flip_dim(avg_waveform_per_tp, param)\n",
    "        euclid_dist = mf.get_Euclidean_dist(avg_waveform_per_tp_flip, param)\n",
    "\n",
    "        centroid_dist, centroid_var = mf.centroid_metrics(euclid_dist, param)\n",
    "\n",
    "        euclid_dist_rc = mf.get_recentered_euclidean_dist(avg_waveform_per_tp_flip, avg_centroid, param)\n",
    "\n",
    "        centroid_dist_recentered = mf.recentered_metrics(euclid_dist_rc)\n",
    "        traj_angle_score, traj_dist_score = mf.dist_angle(avg_waveform_per_tp_flip, param)\n",
    "\n",
    "        # Average Euc Dist\n",
    "        euclid_dist = np.nanmin(euclid_dist[:,param['peak_loc'] - param['waveidx'] == 0, :,:].squeeze(), axis=1)\n",
    "\n",
    "        # TotalScore\n",
    "        include_these_pairs = np.argwhere(euclid_dist < param['max_dist'])  # array indices of pairs to include\n",
    "        include_these_pairs_idx = np.zeros_like(euclid_dist)\n",
    "        include_these_pairs_idx[euclid_dist < param['max_dist']] = 1\n",
    "\n",
    "        # Make a dictionary of score to include\n",
    "        centroid_overlord_score = (centroid_dist_recentered + centroid_var) / 2\n",
    "        waveform_score = (wave_corr_score + wave_mse_score) / 2\n",
    "        trajectory_score = (traj_angle_score + traj_dist_score) / 2\n",
    "\n",
    "        scores_to_include = {'amp_score': amp_score, 'spatial_decay_score': spatial_decay_score, 'centroid_overlord_score': centroid_overlord_score,\n",
    "                            'centroid_dist': centroid_dist, 'waveform_score': waveform_score, 'trajectory_score': trajectory_score}\n",
    "\n",
    "        total_score, predictors = mf.get_total_score(scores_to_include, param)\n",
    "\n",
    "        # Initial thresholding\n",
    "        if (i < niter - 1):\n",
    "            # Get the threshold for a match\n",
    "            thrs_opt = mf.get_threshold(total_score, within_session, euclid_dist, param, is_first_pass=True)\n",
    "\n",
    "            param['n_expected_matches'] = np.sum((total_score > thrs_opt).astype(int))\n",
    "            prior_match = 1 - (param['n_expected_matches'] / len(include_these_pairs))\n",
    "            candidate_pairs = total_score > thrs_opt\n",
    "\n",
    "            drifts, avg_centroid, avg_waveform_per_tp = mf.drift_n_sessions(candidate_pairs, session_switch, avg_centroid, avg_waveform_per_tp, total_score, param)\n",
    "\n",
    "    # Final threshold calculation with debugging\n",
    "    thrs_opt = mf.get_threshold(total_score, within_session, euclid_dist, param, is_first_pass=False)\n",
    "    param['n_expected_matches'] = np.sum((total_score > thrs_opt).astype(int))\n",
    "    prior_match = 1 - (param['n_expected_matches'] / len(include_these_pairs))\n",
    "    \n",
    "    # Debug: Let's examine the total_score and prior_match values\n",
    "    print(f\"Debug patched function:\")\n",
    "    print(f\"include_these_pairs length: {len(include_these_pairs)}\")\n",
    "    print(f\"n_expected_matches: {param['n_expected_matches']}\")\n",
    "    print(f\"prior_match calculation: 1 - ({param['n_expected_matches']} / {len(include_these_pairs)}) = {prior_match}\")\n",
    "    print(f\"prior_match value: {prior_match}\")\n",
    "    print(f\"prior_match type: {type(prior_match)}\")\n",
    "    print(f\"Is prior_match finite: {np.isfinite(prior_match)}\")\n",
    "    print(f\"total_score shape: {total_score.shape}\")\n",
    "    print(f\"include_these_pairs_idx sum: {np.sum(include_these_pairs_idx.astype(bool))}\")\n",
    "    print(f\"total_score[include_these_pairs_idx.astype(bool)] shape: {total_score[include_these_pairs_idx.astype(bool)].shape}\")\n",
    "    \n",
    "    if len(include_these_pairs) > 0:\n",
    "        print(f\"total_score min: {np.min(total_score[include_these_pairs_idx.astype(bool)])}\")\n",
    "        print(f\"total_score max: {np.max(total_score[include_these_pairs_idx.astype(bool)])}\")\n",
    "    \n",
    "    # Original problematic line that we're trying to debug\n",
    "    try:\n",
    "        thrs_opt = np.quantile(total_score[include_these_pairs_idx.astype(bool)], prior_match)\n",
    "        print(f\"Quantile calculation succeeded: {thrs_opt}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Quantile calculation failed: {e}\")\n",
    "        # Let's try to understand what's wrong\n",
    "        if not (0 <= prior_match <= 1):\n",
    "            print(\"prior_match is outside [0,1] range!\")\n",
    "            if prior_match < 0:\n",
    "                print(\"prior_match is negative - using 0.0 instead\")\n",
    "                prior_match = 0.0\n",
    "            elif prior_match > 1:\n",
    "                print(\"prior_match is greater than 1 - using 1.0 instead\")\n",
    "                prior_match = 1.0\n",
    "            \n",
    "            try:\n",
    "                thrs_opt = np.quantile(total_score[include_these_pairs_idx.astype(bool)], prior_match)\n",
    "                print(f\"Quantile calculation with corrected prior_match succeeded: {thrs_opt}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Quantile calculation still failed: {e2}\")\n",
    "                thrs_opt = np.median(total_score[include_these_pairs_idx.astype(bool)])\n",
    "                print(f\"Using median as fallback: {thrs_opt}\")\n",
    "    \n",
    "    candidate_pairs = total_score > thrs_opt\n",
    "    return total_score, candidate_pairs, scores_to_include, predictors\n",
    "\n",
    "# Use the patched function\n",
    "try:\n",
    "    total_score, candidate_pairs, scores_to_include, predictors = patched_extract_metric_scores(\n",
    "        extracted_wave_properties, session_switch, within_session, param, niter=2\n",
    "    )\n",
    "\n",
    "    print(f\"SUCCESS: Number of candidate pairs: {np.sum(candidate_pairs)}\")\n",
    "    print(f\"Scores included: {list(scores_to_include.keys())}\")\n",
    "    print(\"Metric calculation and drift correction complete.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in patched function: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: UnitMatch - Naive Bayes Classification\n",
    "print(\"Running Naive Bayes classification...\")\n",
    "\n",
    "# STEP 5 -- Probability analysis\n",
    "# Get prior probability of being a match\n",
    "prior_match = 1 - (param['n_expected_matches'] / param['n_units']**2)\n",
    "priors = np.array((prior_match, 1-prior_match))\n",
    "\n",
    "print(f\"Prior probability of match: {prior_match:.4f}\")\n",
    "\n",
    "# Construct distributions (kernels) for Naive Bayes Classifier\n",
    "labels = candidate_pairs.astype(int)\n",
    "cond = np.unique(labels)\n",
    "score_vector = param['score_vector']\n",
    "parameter_kernels = np.full((len(score_vector), len(scores_to_include), len(cond)), np.nan)\n",
    "\n",
    "parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, param, add_one=1)\n",
    "\n",
    "# Get probability of each pair being a match\n",
    "probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, param, cond)\n",
    "\n",
    "# Reshape probability matrix\n",
    "output_prob_matrix = probability[:,1].reshape(param['n_units'], param['n_units'])\n",
    "\n",
    "print(\"Naive Bayes classification complete.\")\n",
    "print(f\"Probability matrix shape: {output_prob_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64df43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Evaluate Results and Apply Threshold\n",
    "print(\"Evaluating UnitMatch results...\")\n",
    "\n",
    "# Evaluate output with different thresholds\n",
    "util.evaluate_output(output_prob_matrix, param, within_session, session_switch, match_threshold=0.75)\n",
    "\n",
    "# Set match threshold (you can experiment with different values)\n",
    "match_threshold = param['match_threshold']  # or set your own value, e.g., 0.75\n",
    "\n",
    "# Apply threshold to create binary match matrix\n",
    "output_threshold = np.zeros_like(output_prob_matrix)\n",
    "output_threshold[output_prob_matrix > match_threshold] = 1\n",
    "\n",
    "# Visualize the thresholded matches\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(output_threshold, cmap='Greys')\n",
    "plt.title(f'Unit Matches (threshold = {match_threshold})')\n",
    "plt.xlabel('Unit Index')\n",
    "plt.ylabel('Unit Index')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Count matches\n",
    "n_matches = np.sum(output_threshold) // 2  # Divide by 2 because matrix is symmetric\n",
    "print(f\"Number of putative matches found: {n_matches}\")\n",
    "print(f\"Match threshold used: {match_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Prepare and Launch GUI (Optional)\n",
    "print(\"Preparing data for GUI...\")\n",
    "\n",
    "# Format data for GUI\n",
    "amplitude = extracted_wave_properties['amplitude']\n",
    "spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "wave_idx = extracted_wave_properties['good_wave_idxs']\n",
    "max_site = extracted_wave_properties['max_site']\n",
    "max_site_mean = extracted_wave_properties['max_site_mean']\n",
    "\n",
    "# Process info for GUI\n",
    "gui.process_info_for_GUI(\n",
    "    output_prob_matrix, match_threshold, scores_to_include, total_score, amplitude, spatial_decay,\n",
    "    avg_centroid, avg_waveform, avg_waveform_per_tp, wave_idx, max_site, max_site_mean, \n",
    "    waveform, within_session, channel_pos, clus_info, param\n",
    ")\n",
    "\n",
    "print(\"GUI data preparation complete.\")\n",
    "print(\"To launch the GUI, run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc936b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9: Launch GUI for Manual Curation (Optional)\n",
    "# Uncomment the lines below to run the GUI for manual curation\n",
    "\n",
    "# print(\"Launching UnitMatch GUI...\")\n",
    "# is_match, not_match, matches_GUI = gui.run_GUI()\n",
    "\n",
    "# # If you ran the GUI, curate the matches\n",
    "# matches_curated = util.curate_matches(matches_GUI, is_match, not_match, mode='And')\n",
    "# print(f\"Manual curation complete. Curated matches: {len(matches_curated)}\")\n",
    "\n",
    "print(\"GUI section ready. Uncomment the lines above to run manual curation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b75ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 10: Save Results\n",
    "print(\"Saving UnitMatch results...\")\n",
    "\n",
    "# Get matches from thresholded matrix\n",
    "matches = np.argwhere(output_threshold == 1)\n",
    "\n",
    "# Assign unique IDs to matched units\n",
    "UIDs = aid.assign_unique_id(output_prob_matrix, param, clus_info)\n",
    "\n",
    "# Create output directory with testing suffix\n",
    "unitmatch_output_dir = os.path.join(save_dir, 'unitmatch_results_testing_jf')\n",
    "os.makedirs(unitmatch_output_dir, exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "# NOTE: Change 'matches' to 'matches_curated' if you performed manual curation with the GUI\n",
    "su.save_to_output(\n",
    "    unitmatch_output_dir, \n",
    "    scores_to_include, \n",
    "    matches,  # Use matches_curated if you did manual curation\n",
    "    output_prob_matrix, \n",
    "    avg_centroid, \n",
    "    avg_waveform, \n",
    "    avg_waveform_per_tp, \n",
    "    max_site,\n",
    "    total_score, \n",
    "    output_threshold, \n",
    "    clus_info, \n",
    "    param, \n",
    "    UIDs=UIDs, \n",
    "    matches_curated=None,  # Set to matches_curated if you did manual curation\n",
    "    save_match_table=True\n",
    ")\n",
    "\n",
    "print(f\"Results saved to: {unitmatch_output_dir}\")\n",
    "print(f\"Number of matches saved: {len(matches)}\")\n",
    "print(f\"Unique IDs assigned to {len(UIDs)} units\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== PROCESSING SUMMARY ===\")\n",
    "print(f\"BombCell processed {len(KS_dirs)} sessions\")\n",
    "print(f\"UnitMatch analyzed {param['n_units']} good units\")\n",
    "print(f\"Found {n_matches} putative matches\")\n",
    "print(f\"Results saved to: {unitmatch_output_dir}\")\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
