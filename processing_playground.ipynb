{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fda795b",
   "metadata": {},
   "source": [
    "## Processing playground \n",
    "### tracking neurons across days "
   ]
  },
  {
   "cell_type": "code",
   "id": "cae836c0",
   "metadata": {},
   "outputs": [],
   "source": "# Import necessary libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Enable autoreload like in the demo\n%load_ext autoreload\n%autoreload 2\n\n# Import bombcell like in the demo\nimport bombcell as bc\n\nprint(\"Available bombcell functions:\")\nprint([attr for attr in dir(bc) if not attr.startswith('_')])\n\n# UnitMatch imports\nimport UnitMatchPy.bayes_functions as bf\nimport UnitMatchPy.utils as util\nimport UnitMatchPy.overlord as ov\nimport UnitMatchPy.save_utils as su\nimport UnitMatchPy.GUI as gui\nimport UnitMatchPy.assign_unique_id as aid\nimport UnitMatchPy.default_params as default_params"
  },
  {
   "cell_type": "code",
   "id": "326afb5a",
   "metadata": {},
   "outputs": [],
   "source": "## Step 1: Set up file paths\n# KiloSort directories - USE SAME SESSION TWICE for merge/split testing\nKS_dirs = [r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4',\n           r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/kilosort4']  # Same session repeated\n\n# BombCell output directories - USE SAME SESSION TWICE  \ncustom_bombcell_paths = [r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf',\n                         r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/2023-04-19/cz_npxl_g0/cz_npxl_g0_imec0/bombcell_testing_jf']  # Same session repeated\n\n# Output directory for saving results - add testing suffix\nsave_dir = r'/home/jf5479/cup/Chris/data/cta_backwards/calca_302/unitmatch_output_testing_jf'\nos.makedirs(save_dir, exist_ok=True)\n\nprint(f\"KiloSort directories: {KS_dirs}\")\nprint(f\"BombCell directories: {custom_bombcell_paths}\")\nprint(f\"Output directory: {save_dir}\")\nprint(\"\")\nprint(\"NOTE: Using the SAME session twice to test for merges/splits within the session\")\nprint(\"This is correct for validating UnitMatch merge detection capabilities\")"
  },
  {
   "cell_type": "code",
   "id": "34299cec",
   "metadata": {},
   "outputs": [],
   "source": "## Step 2: Run BombCell quality metrics and extract raw waveforms\nprint(\"Starting BombCell processing...\")\n\n# Process each session with BombCell\nbombcell_results = {}\nfor i, session_dir in enumerate(KS_dirs):\n    print(f\"Processing session {i+1}: {session_dir}\")\n    \n    # Find raw data file (.bin) and meta file (.meta) \n    session_path = Path(session_dir).parent\n    raw_files = list(session_path.glob(\"*.ap.bin\"))\n    meta_files = list(session_path.glob(\"*.ap.meta\"))\n    \n    raw_file = str(raw_files[0]) if raw_files else None\n    meta_file = str(meta_files[0]) if meta_files else None\n    \n    print(f\"  Raw file: {raw_file}\")\n    print(f\"  Meta file: {meta_file}\")\n    \n    # Get UnitMatch-optimized BombCell parameters (includes saveMultipleRaw=True)\n    param = bc.default_parameters.get_unit_match_parameters(session_dir, \n                                                           raw_file=raw_file,  # Provide raw file path\n                                                           meta_file=meta_file,  # Provide meta file path\n                                                           kilosort_version=4)  # Adjust based on your KS version\n    \n    # Speed optimizations but keep 1000 spikes\n    param['computeDistanceMetrics'] = False  # Disable expensive metrics\n    param['computeDrift'] = False\n    param['saveAsTSV'] = True  # Save results in phy-compatible format\n    param['plotGlobal'] = False  # Disable plotting for speed\n    param['plotDetails'] = False  # Disable detailed plots\n    # Keep nRawSpikesToExtract = 1000 (default from get_unit_match_parameters)\n    \n    # Verify raw waveform extraction is enabled\n    print(f\"  Raw data file in param: {param.get('raw_data_file', 'None')}\")\n    print(f\"  extractRaw: {param.get('extractRaw', False)}\")\n    print(f\"  saveMultipleRaw: {param.get('saveMultipleRaw', False)}\")\n    print(f\"  nRawSpikesToExtract: {param.get('nRawSpikesToExtract', 'Unknown')}\")\n    \n    # Set BombCell output directory with testing suffix\n    bc_output_dir = Path(session_dir).parent / 'bombcell_testing_jf'\n    \n    try:\n        # Run BombCell - the function should be imported at top level\n        (quality_metrics, param, unit_type, unit_type_string) = bc.run_bombcell(\n            session_dir, bc_output_dir, param\n        )\n        \n        # Check for NaNs in saved raw waveforms\n        print(f\"  Checking for NaNs in raw waveforms...\")\n        raw_waveforms_dir = bc_output_dir / 'RawWaveforms'\n        if raw_waveforms_dir.exists():\n            npy_files = list(raw_waveforms_dir.glob('*.npy'))\n            nan_files = []\n            total_files = len(npy_files)\n            \n            for npy_file in npy_files:\n                try:\n                    data = np.load(npy_file)\n                    if np.any(np.isnan(data)):\n                        nan_files.append(npy_file.name)\n                except Exception as e:\n                    print(f\"    Error loading {npy_file.name}: {e}\")\n            \n            print(f\"  Raw waveform files: {total_files}\")\n            print(f\"  Files with NaNs: {len(nan_files)}\")\n            if nan_files:\n                print(f\"  NaN files: {nan_files[:5]}...\")  # Show first 5\n        else:\n            print(f\"  ❌ RawWaveforms directory not found!\")\n        \n        bombcell_results[f'session_{i+1}'] = {\n            'quality_metrics': quality_metrics,\n            'unit_type': unit_type,\n            'unit_type_string': unit_type_string,\n            'param': param,\n            'session_dir': session_dir,\n            'bc_output_dir': str(bc_output_dir),\n            'nan_files_count': len(nan_files) if 'nan_files' in locals() else 0\n        }\n        \n        print(f\"BombCell processing complete for session {i+1}\")\n        print(f\"  - Total units: {len(quality_metrics['phy_clusterID'])}\")\n        print(f\"  - Good units: {sum(np.array(unit_type_string) == 'GOOD')}\")\n        print(f\"  - Results saved to: {bc_output_dir}\")\n        \n    except Exception as e:\n        print(f\"Error processing session {i+1}: {e}\")\n        import traceback\n        traceback.print_exc()\n        continue\n\nprint(\"BombCell processing completed for all sessions.\")\n\n# Summary of NaN detection\nprint(\"\\n=== NaN Detection Summary ===\")\nfor session, result in bombcell_results.items():\n    nan_count = result.get('nan_files_count', 0)\n    print(f\"{session}: {nan_count} files with NaNs\")"
  },
  {
   "cell_type": "code",
   "id": "4c8dca3f",
   "metadata": {},
   "outputs": [],
   "source": "## Step 3: Prepare data for UnitMatch\nprint(\"Preparing data for UnitMatch...\")\n\n# Since we know the raw waveforms are saved, let's directly use the paths\nprint(\"Using predefined BombCell paths...\")\nbc_output_dirs = custom_bombcell_paths\n\n# Check if directories exist and have RawWaveforms\nfor i, bc_dir in enumerate(bc_output_dirs):\n    raw_waveforms_dir = Path(bc_dir) / 'RawWaveforms'\n    if raw_waveforms_dir.exists():\n        npy_files = list(raw_waveforms_dir.glob('*.npy'))\n        print(f\"  Session {i+1}: {len(npy_files)} .npy files found in {raw_waveforms_dir}\")\n    else:\n        print(f\"  Session {i+1}: RawWaveforms directory not found at {raw_waveforms_dir}\")\n\n# Get default UnitMatch parameters\nparam = default_params.get_default_param()\n\n# Set up paths for UnitMatch - using the directories defined above\nparam['KS_dirs'] = KS_dirs\n\nprint(f\"BombCell output directories: {bc_output_dirs}\")\n\n# Manually construct the correct paths since util.paths_from_KS isn't working properly\nwave_paths = []\nunit_label_paths = []\nchannel_pos = []\n\nfor i, (ks_dir, bc_dir) in enumerate(zip(KS_dirs, bc_output_dirs)):\n    # Raw waveforms path\n    wave_path = Path(bc_dir) / 'RawWaveforms'\n    wave_paths.append(str(wave_path))\n    \n    # Unit label path - point to the specific TSV file\n    unit_label_path = Path(bc_dir) / 'cluster_bc_unitType.tsv'\n    unit_label_paths.append(str(unit_label_path))\n    \n    # Channel positions from KiloSort\n    channel_pos_path = Path(ks_dir) / 'channel_positions.npy'\n    if channel_pos_path.exists():\n        channel_pos.append(np.load(channel_pos_path))\n    else:\n        print(f\"WARNING: channel_positions.npy not found in {ks_dir}\")\n\nprint(f\"Raw waveform paths: {wave_paths}\")\nprint(f\"Unit label paths: {unit_label_paths}\")\nprint(f\"Channel positions loaded: {len(channel_pos)} sessions\")\n\n# Verify the unit label files exist\nfor i, unit_label_path in enumerate(unit_label_paths):\n    if Path(unit_label_path).exists():\n        print(f\"  Session {i+1}: Unit label file found: {unit_label_path}\")\n    else:\n        print(f\"  Session {i+1}: Unit label file NOT found: {unit_label_path}\")\n\n# Get probe geometry\nif len(channel_pos) > 0:\n    param = util.get_probe_geometry(channel_pos[0], param)\n    print(\"Data preparation for UnitMatch complete.\")\nelse:\n    print(\"ERROR: No channel positions loaded\")"
  },
  {
   "cell_type": "code",
   "id": "20265e9f",
   "metadata": {},
   "outputs": [],
   "source": "## Step 4: Run UnitMatch - Data Loading and Parameter Extraction\n\n# Check if previous step completed successfully\nif 'wave_paths' not in locals():\n    print(\"ERROR: wave_paths not defined - UnitMatch preparation failed\")\n    print(\"Please run the previous cell (Step 3) successfully first\")\n    print(\"Current local variables:\", [var for var in locals().keys() if not var.startswith('_')])\nelse:\n    print(\"Starting UnitMatch processing...\")\n\n    # STEP 0 -- Data preparation\n    print(\"Loading good waveforms...\")\n    waveform, session_id, session_switch, within_session, good_units, param = util.load_good_waveforms(\n        wave_paths, unit_label_paths, param, good_units_only=True\n    ) \n\n    # Fix channel positions - convert 2D to 3D if needed\n    for i, ch_pos in enumerate(channel_pos):\n        if ch_pos.shape[1] == 2:\n            # Add z=0 dimension for 2D Neuropixels probes\n            print(f\"Converting session {i+1} channel positions from 2D to 3D\")\n            channel_pos[i] = np.column_stack([ch_pos, np.zeros(ch_pos.shape[0])])\n        print(f\"Session {i+1} channel positions shape: {channel_pos[i].shape}\")\n\n    # You may need to set peak location if it's not automatically detected correctly\n    # param['peak_loc'] = # set as a value if the peak location is NOT ~ half the spike width\n\n    # Create clus_info containing all unit id/session related info\n    clus_info = {\n        'good_units': good_units, \n        'session_switch': session_switch, \n        'session_id': session_id, \n        'original_ids': np.concatenate(good_units)\n    }\n\n    print(f\"Total number of good units: {param['n_units']}\")\n    print(f\"Number of sessions: {len(KS_dirs)}\")\n    \n    # Check for NaNs in waveform data before processing\n    total_nans = np.sum(np.isnan(waveform))\n    total_elements = waveform.size\n    print(f\"Waveform data quality check: {total_nans}/{total_elements} NaN values ({100*total_nans/total_elements:.2f}%)\")\n    \n    if total_nans > 0:\n        print(\"WARNING: Found NaN values in waveform data - this may cause metric calculation warnings\")\n        # Find which units have NaNs\n        units_with_nans = []\n        for i in range(waveform.shape[0]):\n            if np.any(np.isnan(waveform[i])):\n                units_with_nans.append(i)\n        print(f\"Units with NaN waveforms: {len(units_with_nans)} out of {waveform.shape[0]}\")\n\n    # STEP 1 -- Extract parameters from waveforms\n    print(\"Extracting waveform parameters...\")\n    print(\"(RuntimeWarnings about NaN slices are expected and handled by UnitMatch)\")\n    extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, param)\n    print(\"Parameter extraction complete.\")"
  },
  {
   "cell_type": "code",
   "id": "0ee5de23",
   "metadata": {},
   "outputs": [],
   "source": "## Step 5: UnitMatch - Metric Calculation and Drift Correction\nprint(\"Calculating similarity metrics and applying drift correction...\")\n\n# Temporarily patch the UnitMatch overlord function to debug the quantile issue\nimport UnitMatchPy.overlord as ov_original\nimport UnitMatchPy.metric_functions as mf\n\ndef patched_extract_metric_scores(extracted_wave_properties, session_switch, within_session, param, niter=2):\n    \"\"\"Patched version with debugging for quantile issue\"\"\"\n    import numpy as np\n    \n    # Unpack needed arrays from the ExtractedWaveProperties dictionary\n    amplitude = extracted_wave_properties['amplitude']\n    spatial_decay = extracted_wave_properties['spatial_decay']\n    spatial_decay_fit = extracted_wave_properties['spatial_decay_fit']\n    avg_waveform = extracted_wave_properties['avg_waveform']\n    avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n    avg_centroid = extracted_wave_properties['avg_centroid']\n\n    # These scores are NOT affected by the drift correction\n    amp_score = mf.get_simple_metric(amplitude)\n    spatial_decay_score = mf.get_simple_metric(spatial_decay)\n    spatial_decay_fit_score = mf.get_simple_metric(spatial_decay_fit, outlier=True)\n    wave_corr_score = mf.get_wave_corr(avg_waveform, param)\n    wave_mse_score = mf.get_waveforms_mse(avg_waveform, param)\n\n    # Affected by drift\n    for i in range(niter):\n        avg_waveform_per_tp_flip = mf.flip_dim(avg_waveform_per_tp, param)\n        euclid_dist = mf.get_Euclidean_dist(avg_waveform_per_tp_flip, param)\n\n        centroid_dist, centroid_var = mf.centroid_metrics(euclid_dist, param)\n\n        euclid_dist_rc = mf.get_recentered_euclidean_dist(avg_waveform_per_tp_flip, avg_centroid, param)\n\n        centroid_dist_recentered = mf.recentered_metrics(euclid_dist_rc)\n        traj_angle_score, traj_dist_score = mf.dist_angle(avg_waveform_per_tp_flip, param)\n\n        # Average Euc Dist\n        euclid_dist = np.nanmin(euclid_dist[:,param['peak_loc'] - param['waveidx'] == 0, :,:].squeeze(), axis=1)\n\n        # TotalScore\n        include_these_pairs = np.argwhere(euclid_dist < param['max_dist'])  # array indices of pairs to include\n        include_these_pairs_idx = np.zeros_like(euclid_dist)\n        include_these_pairs_idx[euclid_dist < param['max_dist']] = 1\n\n        # Make a dictionary of score to include\n        centroid_overlord_score = (centroid_dist_recentered + centroid_var) / 2\n        waveform_score = (wave_corr_score + wave_mse_score) / 2\n        trajectory_score = (traj_angle_score + traj_dist_score) / 2\n\n        scores_to_include = {'amp_score': amp_score, 'spatial_decay_score': spatial_decay_score, 'centroid_overlord_score': centroid_overlord_score,\n                            'centroid_dist': centroid_dist, 'waveform_score': waveform_score, 'trajectory_score': trajectory_score}\n\n        total_score, predictors = mf.get_total_score(scores_to_include, param)\n\n        # Initial thresholding\n        if (i < niter - 1):\n            # Get the threshold for a match\n            thrs_opt = mf.get_threshold(total_score, within_session, euclid_dist, param, is_first_pass=True)\n\n            param['n_expected_matches'] = np.sum((total_score > thrs_opt).astype(int))\n            prior_match = 1 - (param['n_expected_matches'] / len(include_these_pairs))\n            candidate_pairs = total_score > thrs_opt\n\n            drifts, avg_centroid, avg_waveform_per_tp = mf.drift_n_sessions(candidate_pairs, session_switch, avg_centroid, avg_waveform_per_tp, total_score, param)\n\n    # Final threshold calculation with debugging\n    thrs_opt = mf.get_threshold(total_score, within_session, euclid_dist, param, is_first_pass=False)\n    param['n_expected_matches'] = np.sum((total_score > thrs_opt).astype(int))\n    prior_match = 1 - (param['n_expected_matches'] / len(include_these_pairs))\n    \n    # Debug: Let's examine the total_score and prior_match values\n    print(f\"Debug patched function:\")\n    print(f\"include_these_pairs length: {len(include_these_pairs)}\")\n    print(f\"n_expected_matches: {param['n_expected_matches']}\")\n    print(f\"prior_match calculation: 1 - ({param['n_expected_matches']} / {len(include_these_pairs)}) = {prior_match}\")\n    print(f\"prior_match value: {prior_match}\")\n    print(f\"prior_match type: {type(prior_match)}\")\n    print(f\"Is prior_match finite: {np.isfinite(prior_match)}\")\n    print(f\"total_score shape: {total_score.shape}\")\n    print(f\"include_these_pairs_idx sum: {np.sum(include_these_pairs_idx.astype(bool))}\")\n    print(f\"total_score[include_these_pairs_idx.astype(bool)] shape: {total_score[include_these_pairs_idx.astype(bool)].shape}\")\n    \n    if len(include_these_pairs) > 0:\n        print(f\"total_score min: {np.min(total_score[include_these_pairs_idx.astype(bool)])}\")\n        print(f\"total_score max: {np.max(total_score[include_these_pairs_idx.astype(bool)])}\")\n    \n    # Original problematic line that we're trying to debug\n    try:\n        thrs_opt = np.quantile(total_score[include_these_pairs_idx.astype(bool)], prior_match)\n        print(f\"Quantile calculation succeeded: {thrs_opt}\")\n    except Exception as e:\n        print(f\"Quantile calculation failed: {e}\")\n        # Let's try to understand what's wrong\n        if not (0 <= prior_match <= 1):\n            print(\"prior_match is outside [0,1] range!\")\n            if prior_match < 0:\n                print(\"prior_match is negative - using 0.0 instead\")\n                prior_match = 0.0\n            elif prior_match > 1:\n                print(\"prior_match is greater than 1 - using 1.0 instead\")\n                prior_match = 1.0\n            \n            try:\n                thrs_opt = np.quantile(total_score[include_these_pairs_idx.astype(bool)], prior_match)\n                print(f\"Quantile calculation with corrected prior_match succeeded: {thrs_opt}\")\n            except Exception as e2:\n                print(f\"Quantile calculation still failed: {e2}\")\n                thrs_opt = np.median(total_score[include_these_pairs_idx.astype(bool)])\n                print(f\"Using median as fallback: {thrs_opt}\")\n    \n    candidate_pairs = total_score > thrs_opt\n    return total_score, candidate_pairs, scores_to_include, predictors\n\n# Use the patched function\ntry:\n    total_score, candidate_pairs, scores_to_include, predictors = patched_extract_metric_scores(\n        extracted_wave_properties, session_switch, within_session, param, niter=2\n    )\n\n    print(f\"SUCCESS: Number of candidate pairs: {np.sum(candidate_pairs)}\")\n    print(f\"Scores included: {list(scores_to_include.keys())}\")\n    print(\"Metric calculation and drift correction complete.\")\n    \nexcept Exception as e:\n    print(f\"ERROR in patched function: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0fde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: UnitMatch - Naive Bayes Classification\n",
    "print(\"Running Naive Bayes classification...\")\n",
    "\n",
    "# STEP 5 -- Probability analysis\n",
    "# Get prior probability of being a match\n",
    "prior_match = 1 - (param['n_expected_matches'] / param['n_units']**2)\n",
    "priors = np.array((prior_match, 1-prior_match))\n",
    "\n",
    "print(f\"Prior probability of match: {prior_match:.4f}\")\n",
    "\n",
    "# Construct distributions (kernels) for Naive Bayes Classifier\n",
    "labels = candidate_pairs.astype(int)\n",
    "cond = np.unique(labels)\n",
    "score_vector = param['score_vector']\n",
    "parameter_kernels = np.full((len(score_vector), len(scores_to_include), len(cond)), np.nan)\n",
    "\n",
    "parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, param, add_one=1)\n",
    "\n",
    "# Get probability of each pair being a match\n",
    "probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, param, cond)\n",
    "\n",
    "# Reshape probability matrix\n",
    "output_prob_matrix = probability[:,1].reshape(param['n_units'], param['n_units'])\n",
    "\n",
    "print(\"Naive Bayes classification complete.\")\n",
    "print(f\"Probability matrix shape: {output_prob_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64df43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7: Evaluate Results and Apply Threshold\n",
    "print(\"Evaluating UnitMatch results...\")\n",
    "\n",
    "# Evaluate output with different thresholds\n",
    "util.evaluate_output(output_prob_matrix, param, within_session, session_switch, match_threshold=0.75)\n",
    "\n",
    "# Set match threshold (you can experiment with different values)\n",
    "match_threshold = param['match_threshold']  # or set your own value, e.g., 0.75\n",
    "\n",
    "# Apply threshold to create binary match matrix\n",
    "output_threshold = np.zeros_like(output_prob_matrix)\n",
    "output_threshold[output_prob_matrix > match_threshold] = 1\n",
    "\n",
    "# Visualize the thresholded matches\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(output_threshold, cmap='Greys')\n",
    "plt.title(f'Unit Matches (threshold = {match_threshold})')\n",
    "plt.xlabel('Unit Index')\n",
    "plt.ylabel('Unit Index')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Count matches\n",
    "n_matches = np.sum(output_threshold) // 2  # Divide by 2 because matrix is symmetric\n",
    "print(f\"Number of putative matches found: {n_matches}\")\n",
    "print(f\"Match threshold used: {match_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Prepare and Launch GUI (Optional)\n",
    "print(\"Preparing data for GUI...\")\n",
    "\n",
    "# Format data for GUI\n",
    "amplitude = extracted_wave_properties['amplitude']\n",
    "spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "wave_idx = extracted_wave_properties['good_wave_idxs']\n",
    "max_site = extracted_wave_properties['max_site']\n",
    "max_site_mean = extracted_wave_properties['max_site_mean']\n",
    "\n",
    "# Process info for GUI\n",
    "gui.process_info_for_GUI(\n",
    "    output_prob_matrix, match_threshold, scores_to_include, total_score, amplitude, spatial_decay,\n",
    "    avg_centroid, avg_waveform, avg_waveform_per_tp, wave_idx, max_site, max_site_mean, \n",
    "    waveform, within_session, channel_pos, clus_info, param\n",
    ")\n",
    "\n",
    "print(\"GUI data preparation complete.\")\n",
    "print(\"To launch the GUI, run the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc936b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9: Launch GUI for Manual Curation (Optional)\n",
    "# Uncomment the lines below to run the GUI for manual curation\n",
    "\n",
    "# print(\"Launching UnitMatch GUI...\")\n",
    "# is_match, not_match, matches_GUI = gui.run_GUI()\n",
    "\n",
    "# # If you ran the GUI, curate the matches\n",
    "# matches_curated = util.curate_matches(matches_GUI, is_match, not_match, mode='And')\n",
    "# print(f\"Manual curation complete. Curated matches: {len(matches_curated)}\")\n",
    "\n",
    "print(\"GUI section ready. Uncomment the lines above to run manual curation.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "c2b75ed0",
   "metadata": {},
   "outputs": [],
   "source": "## Step 10: Save Results\nprint(\"Saving UnitMatch results...\")\n\n# Get matches from thresholded matrix\nmatches = np.argwhere(output_threshold == 1)\n\n# Assign unique IDs to matched units\nUIDs = aid.assign_unique_id(output_prob_matrix, param, clus_info)\n\n# Create output directory with testing suffix\nunitmatch_output_dir = os.path.join(save_dir, 'unitmatch_results_testing_jf')\nos.makedirs(unitmatch_output_dir, exist_ok=True)\n\n# Save results\n# NOTE: Change 'matches' to 'matches_curated' if you performed manual curation with the GUI\nsu.save_to_output(\n    unitmatch_output_dir, \n    scores_to_include, \n    matches,  # Use matches_curated if you did manual curation\n    output_prob_matrix, \n    avg_centroid, \n    avg_waveform, \n    avg_waveform_per_tp, \n    max_site,\n    total_score, \n    output_threshold, \n    clus_info, \n    param, \n    UIDs=UIDs, \n    matches_curated=None,  # Set to matches_curated if you did manual curation\n    save_match_table=True\n)\n\nprint(f\"Results saved to: {unitmatch_output_dir}\")\nprint(f\"Number of matches saved: {len(matches)}\")\nprint(f\"Unique IDs assigned to {len(UIDs)} units\")\n\n# Print summary\nprint(\"\\n=== PROCESSING SUMMARY ===\")\nprint(f\"BombCell processed {len(KS_dirs)} sessions\")\nprint(f\"UnitMatch analyzed {param['n_units']} good units\")\nprint(f\"Found {n_matches} putative matches\")\nprint(f\"Results saved to: {unitmatch_output_dir}\")\nprint(\"Processing complete!\")"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}